{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda2/lib/python2.7/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "/home/ivan/miniconda2/lib/python2.7/site-packages/IPython/utils/path.py:282: UserWarning: locate_profile has moved to the IPython.paths module\n",
      "  warn(\"locate_profile has moved to the IPython.paths module\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{u'scroll': True,\n",
       " u'start_slideshow_at': 'selected',\n",
       " u'theme': 'sky',\n",
       " u'transition': 'zoom'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.utils.path import locate_profile\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 6. Hierarchical matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture \n",
    "- Complexity analysis of the FMM \n",
    "- Connection between separability and low-rank approximation\n",
    "- Algebraic interpretation of the Barnes-Hut method (H-matrices)\n",
    "- Algebraic interpretation of the FMM method (H^2-matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "- Finalizing the FMM (multipole, local)\n",
    "- Estimates for asymptotically smooth functions\n",
    "- How we construct H-matrix approximation in a \"black-box\" way\n",
    "- Maxvol algorithm and adaptive cross approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dense matrix-by-vector product\n",
    "\n",
    "$$V_i = \\sum_{j} K_{ij} q_j, \\quad j = 1, \\ldots, N, \\quad i = 1, \\ldots, M.$$\n",
    "\n",
    "It happens when:\n",
    "\n",
    "- $N$-body: $V_i = \\sum_{j} q_j K(x_i, y_j)$.\n",
    "- collocation: $V_i = \\sum_{j} q_j \\int_{supp v_j} K(x_i, y) v_j(y) dy$.\n",
    "- Galerkin: $V_i = \\sum_{j} q_j \\int_{supp v_i} \\int_{supp v_j} K(x, y) v_i(x) v_j(y) dx dy$.\n",
    "\n",
    "Each $(i, j)$ is associated as a pair of points $(x_i, y_j)$ in $\\mathbb{R}^d, d = 1, 2, 3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General scheme \n",
    "\n",
    "Given the set of sources $x_i, \\quad i = 1, \\ldots N$ and the set of receivers $y_j, \\quad  j = 1, \\ldots, M$. \n",
    "\n",
    "- Construct a cluster tree for sources with nodes $s$\n",
    "- Construct a cluster tree for receivers with nodes $t$\n",
    "- Based on the separability criteria between nodes $(t, s)$ create **interaction list**,\n",
    "  which is a set of pairs $(t, s)$. The number of such pairs is $\\mathcal{O}(N+M)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main requirement\n",
    "\n",
    "The main requirement on the \"far zone\" and the \"Kernel\" is that\n",
    "\n",
    "If $y$ is closer to $z$, rather than to $x$, then\n",
    "\n",
    "$$K(x, y) \\approx K'_N(x, y) \\approx \\sum_{l=0}^N \\sum_{m \\in M_l} t_{lm}(y, z) T_{lm}(x, z),$$\n",
    "\n",
    "and the error bound looks like\n",
    "\n",
    "$$\\Vert K(x, y) - K'(x, y) \\Vert \\leq c_N \\left( \\frac{\\Vert y - z \\Vert}{\\Vert x - z \\Vert}\\right)^N.$$\n",
    "\n",
    "On the other hand, if $y$ is closer to $x$ rather than $z$ a **dual decomposition** holds.\n",
    "\n",
    "\n",
    "$$K(x, y) \\approx K''_N(x, y)  \\sum_{l=0}^N \\sum_{m \\in M_l} t_{lm}(x, z) T_{lm}(y, z),$$\n",
    "\n",
    "with a bound\n",
    "$$\\Vert K(x, y) - K''(x, y) \\Vert \\leq c_N \\left( \\frac{\\Vert x - z \\Vert}{\\Vert y - z \\Vert}\\right)^N.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Far field\n",
    "\n",
    "The **far field** of the set $Y$ with a center $z \\in Y$ \n",
    "$\\mathcal{F}_q(Y, z)$ is defined as\n",
    "\n",
    "$$\\mathcal{F}_q(Y, z) = \\{x: \\Vert x - z \\Vert \\leq  \\frac{\\Vert y - z \\Vert}{q}, \\forall y \\in Y \\}$$\n",
    "\n",
    "**Main lemma**\n",
    "\n",
    "If $y \\in Y$ and $x \\in \\mathcal{F}_q(Y, z)$ then\n",
    "\n",
    "$$|K(x, y) - K'_N(x,y)| \\leq c_N q^N.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Far field approximation\n",
    "In the far field $x \\in \\mathcal{F}_q(Y, z)$ \n",
    "the field of point charges\n",
    "\n",
    "$$f(x) = \\sum_j q_j K(x, y_j)$$ \n",
    "is approximated by the **multipole expansion**\n",
    "\n",
    "$$M(x) = \\sum_{l=0}^N \\sum_{m \\in M_l} M_{lm} T_{lm}(x, z),$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "  M_{lm} = \\sum_{j} t_{lm}(y_j, z) q_j.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local expansion\n",
    "\n",
    "If in the point $x \\in Y$ the field from charges $y_j \\in \\mathcal{F}_q(Y, z)$ in the **far field** is calculated,\n",
    "\n",
    "it is then approximated by the **local expansion**\n",
    "\n",
    "$$L(x) = \\sum_{l=0}^N \\sum_{m \\in \\mathcal{M}_l} L_{lm} t_{lm}(x, z), \\quad  L_{lm} = \\sum_j  T_{lm}(y_j, z) q_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of local & multipole expansion\n",
    "\n",
    "For the specific kernel, one can write down the specific expansions (you need two).\n",
    "Introduce spherical coordinates with a center in $z$. The point $x$ has coordinates $(r_x, \\phi_x, \\theta_x)$, \n",
    "the point $y$ has coordinates $(r_y, \\phi_y, \\theta_y)$.\n",
    "\n",
    "$$K(x, y) = \\frac{1}{\\Vert x - y \\Vert}.$$\n",
    "\n",
    "Then, $$M(x) = \\sum_{l=0}^N \\sum_{m=-l}^{l} M_{lm} \\frac{1}{r^{l+1}_x} Y^m_l(\\phi_x, \\theta_x), \n",
    "\\quad L(x) = \\sum_{l=0}^N \\sum_{m=-l}^l L_{lm} r^l_x Y^m_{l}(\\phi_x, \\theta_x),$$\n",
    "\n",
    "where $Y^m_l$ are **spherical harmonics.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Helmholtz Kernel\n",
    "\n",
    "For the Helmholtz kernel $K(x, y) = \\frac{e^{ikr}}{r}, \\quad r = \\Vert x - y \\Vert,$\n",
    "the expansions take the form\n",
    "\n",
    "$$M(x) = \\sum_{l=0}^N \\sum_{m=-l}^l M_{lm} H^{(1)}_l(k r_x) Y^m_l(\\theta_x, \\phi_x), \\quad \n",
    " L(x) = \\sum_{l=0}^N \\sum_{m=-l}^l L_{lm} J^{(1)}_l(k r_x) Y^{m}_l(\\theta_x, \\phi_x),$$\n",
    " where $H^{(1)}$ and $J^{(1)}$ are **Hankel** and **Bessel** functions of the first kind.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing the bounds\n",
    "\n",
    "The class of **asymptotically smooth** functions. The function $f$ of many variables is said to be **asymptotically smooth** if \n",
    "\n",
    "$$\\Vert \\partial^{p} f(x, y) \\Vert \\leq c_p \\Vert x - y \\Vert^{(g-p)},$$\n",
    "\n",
    "with a \"reasonable\" bound on $c_p$: \n",
    "    $$c_p \\leq c d^p p!.$$\n",
    "    For $f(x, y) = \\frac{1}{\\Vert x - y \\Vert}$ we have $g = 1$ and $c_p \\leq 4^p p!$ (more accurate estimates give $c_p \\leq 2^p$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using the bounds\n",
    "\n",
    "The main result is the **rank bound** for matrices generated by asymptotically smooth functions.\n",
    "\n",
    "For any two separated sets of sources and receivers such that  $y_j$ are located inside a cube\n",
    "\n",
    "$$C = \\{y: \\Vert y - \\eta \\Vert_{\\infty} \\leq a/2 \\}.$$\n",
    "\n",
    "and $x_i$ lies inside the set\n",
    "\n",
    "$$\\{x : \\Vert x - y \\Vert_{\\infty}  \\geq \\sigma a, \\quad \\forall y \\in C \\}, $$\n",
    "\n",
    "then the interaction matrix \n",
    "\n",
    "$$A = [A_{ij}], \\quad A_{ij} = K(x_i, y_j)$$\n",
    "\n",
    "can be written as \n",
    "\n",
    "$$A = T_p + R_p, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathrm{rank}(R_p) \\leq p^m, \\quad \\Vert R_p \\Vert^2_F \\leq n_x n_y c^2 (\\sigma a)^{2g} \\left(\\frac{dm}{2 \\sigma}\\right)^{2p}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What does it mean\n",
    "\n",
    "$$A = T_p + R_p, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathrm{rank}(R_p) \\leq p^m, \\quad \\Vert T_p \\Vert^2_F \\leq n_x n_y c^2 (\\sigma a)^{2g} \\left(\\frac{dm}{2 \\sigma}\\right)^{2p}.$$\n",
    "\n",
    "It means, that if \n",
    "\n",
    "$$\\frac{dm}{2 \\sigma} < 1 $$\n",
    "\n",
    "(i.e., the cubes are sufficiently separated), the matrix can be approximated by a **low-rank matrix.**\n",
    "\n",
    "This is a **general result** which holds for a class of matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical algorithm\n",
    "\n",
    "The partioning of the matrix can be done using **geometric information** about the sources and receivers.\n",
    "\n",
    "But who do we compute the approximation?\n",
    "\n",
    "<img src=\"pic/h-matrix.jpg\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## H-matrices\n",
    "\n",
    "In the hiearchical matrices we compute the **low-rank approximation** independently.\n",
    "\n",
    "The problem is that the largest block has size $\\mathcal{O}(N) \\times \\mathcal{O}(N)$.\n",
    "\n",
    "The best rank-$r$ approximation can be computed by SVD,\n",
    "\n",
    "but the complexity of the SVD is $\\mathcal{O}(N^3)$.\n",
    "\n",
    "Can we do better than SVD?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remembering the NLA: Skeleton decomposition\n",
    "\n",
    "Even the computation of all the entries of a $N \\times N$ matrix takes $\\mathcal{O}(N^2)$ operations.\n",
    "\n",
    "We do not want that.\n",
    "\n",
    "Instead, we have an apriori knowledge of the fact that  the matrix is (approximately) **low-rank**.\n",
    "\n",
    "The skeleton decomposition allows to recover a rank-$r$ entries for $r$ columns and $r$ rows.\n",
    "\n",
    "<img src='pic/cross-pic.png' width=80% /img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skeleton decomposition\n",
    "\n",
    "The skeleton decomposition has the form\n",
    "\n",
    "$$A = C\\widehat{A}^{-1} R,$$\n",
    "\n",
    "where $C$ is some columns of $A$, $R$ are some rows of $A$ and $\\widehat{A}$ is the submatrix on the intersection.\n",
    "\n",
    "It means, that rank-$r$ matrix can be exactly recovered from $r$ columns and $r$ rows, and only $2Nr$ elements need to be computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to select a good submatrix\n",
    "\n",
    "In practice, if the submatrix is **bad**, the approximation can be **bad**.n{\n",
    "\n",
    "For example, for a matrix\n",
    "\n",
    "$$\n",
    "   A = \\begin{bmatrix}\n",
    "   1 & 1 & 2 \\\\\n",
    "   1 & 1.001 & 3  \\\\\n",
    "   1 & 1    & 4 \\\\\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The submatrix located in first two rows and two columns is really bad for rank-$2$ approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to select a good submatrix(2)\n",
    "\n",
    "The submatrix selection is an **art**: for any strategy you can design  a matrix that the strategy will fail.\n",
    "\n",
    "In practice, however, the matrices come from a certain class, where the submatrix selection is possible.\n",
    "\n",
    "\n",
    "The **maxvol** principle (Goreinov & Tyrtyshnikov) gives the **existence** result for such a submatrix:\n",
    "\n",
    "If $\\widehat{A}$ has the largest volume (the volume of the matrix is defined as an absolute value of the determinant) \n",
    "\n",
    "Then\n",
    "\n",
    "$$\\Vert A - A_{skel} \\Vert \\leq (r + 1) \\sigma_{r+1}, $$\n",
    "\n",
    "where $\\sigma_{r+1}$ is the $(r+1)$-th singular value of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol-iteration\n",
    "\n",
    "Then the simplest adaptive strategy for column/row selection is the **alternating optimization**.\n",
    "\n",
    "We take $r$ columns from the matrix, compute them, \n",
    "\n",
    "and find the **maximum volume** in those columns; \n",
    "\n",
    "Compute the new rows, find the maximal volume and so on.\n",
    "\n",
    "In this iteration the **volume** never decreases, thus the quality of the approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Illustration\n",
    " \n",
    "As an  illustration, consider the case $r = 1$.\n",
    "\n",
    "Then we need to find maximal absolute value, actually we need to find something that is not small.\n",
    "\n",
    "Then we do **block coordinate descent**\n",
    "\n",
    "$$i_* = \\arg \\max_i A_{ij_*}, \\quad j_* = \\arg \\max_j A_{i_* j}$$\n",
    "\n",
    "For a general rank-$r$ case we blocks of rows and columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final remark\n",
    "\n",
    "But how to find a maximal-volume submatrix in a tall matrix?. \n",
    "\n",
    "Let $A$ be $n \\times r$ matrix.\n",
    "m\n",
    "The total number of $r \\times r$ submatrices is $\\mathcal{O}(n^r)$, \n",
    "which is clearly intractable for $r > 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol algorithm: initialization \n",
    "\n",
    "The maxvol algorithm is a **greedy optimization** algorithm for searching for a good submatrix in a tall matrix.\n",
    "\n",
    "Let $A$ be $n \\times r$, $n \\gg r$.\n",
    "\n",
    "**Initialization:** We find some \"sufficiently good\" submatrix $\\widehat{A}$ in $A$. We can do it by Gaussian elimination with column pivoting (please check)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol algorithm: iteration step\n",
    "\n",
    "The iteration step is as follows.\n",
    "\n",
    "We order the rows of the matrix in such a way that\n",
    "\n",
    "$$\n",
    "A = P\\begin{bmatrix}\n",
    "\\widehat{A} \\\\\n",
    "B \n",
    "\\end{bmatrix},$$ where\n",
    "\n",
    "$P$ is the permutation matrix (it permutes the rows of the matrix).\n",
    "\n",
    "Then, \n",
    "\n",
    "$$A \\widehat{A}^{-1} = P \\begin{bmatrix} I \\\\\n",
    "Z \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Note that the ratio of the determinants did not change, so the position of the **maximum volume submatrix is the same**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol algorithm: crucial step\n",
    "We need to find the **maxvol algorithm** in the matrix\n",
    "$$A \\widehat{A}^{-1} = P \\begin{bmatrix} I \\\\\n",
    "Z \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We use the **greedy approach** by increasing the determinant by **permuting two rows** at a time.\n",
    "\n",
    "The algebraic structures helps us to efficiently find such permutation at **very cost.**\n",
    "\n",
    "Find \n",
    "$$\n",
    "(i_*, j_) = \\arg \\max_{i, j} |Z_{ij}|.$$\n",
    "\n",
    "Swap the $j$-th row and the $i$-th row.\n",
    "\n",
    "Then the matrix will be\n",
    "\n",
    "$$A \\widehat{A}^{-1} = P' \\begin{bmatrix} \\begin{bmatrix}1 & \\ldots \\\\\n",
    "                                          \\ldots & z & \\ldots \\\\ \n",
    "                                          \\ldots & \\ldots & 1 \n",
    "                                          \\end{bmatrix}\n",
    "                                          \\\\\n",
    "                                          Z' \n",
    "                                          \\end{bmatrix},$$\n",
    "                                          \n",
    "i.e. the top submatrix differs only in one row from the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The result of the update\n",
    "\n",
    "As the result of the update, \n",
    "\n",
    "we have\n",
    "\n",
    "$$\\det Z_{new} = \\det (I - e_j e^{\\top}_j + z_i e^{\\top}_j) = \\det(I + (z_i - e_j) e^{\\top}_j) = 1 + (e_j, z_i - e_j) = 1  + z_{ij} - 1 = z_{ij}.$$\n",
    "\n",
    "Thus, if $|Z_{ij}| > 1$ the new determinant is greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final scheme of the algorithm\n",
    "\n",
    "1. Initialize the submatrix\n",
    "2. Permute rows to make this submatrix the first one\n",
    "3. multiply by the inverse of $\\widehat{A}^{-1}$ from the right ($\\mathcal{O}(nr^2 + r^3)$ operations)\n",
    "4. Find $(i_*, j_*) = \\arg \\max_{ij} |Z_{ij}|$\n",
    "5. If $|Z_{i_*, j_*}| < 1 + \\delta$, stop\n",
    "7. Otherwise, permute $i_*$, and $j_*$ rows and go to Step 3.\n",
    "\n",
    "The final complexity is $\\mathcal{O}(Inr^2)$ operations. We can make it $\\mathcal{O}(I nr + nr^2)$, where $I$ is the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol algorithm: efficient implementation\n",
    "\n",
    "The efficient implementation is based on the inversion of the matrix after permuting two rows.\n",
    "\n",
    "In fact, we only need to recompute $A \\widehat{A}^{-1}$.\n",
    "\n",
    "Note that \n",
    "$$A_{new} = A + e_i (e_j - e_i)^{\\top} A + e_j (e_i - e^{\\top}_j) A,$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\widehat{A}_{new} = \\widehat{A} + e_i (e_j - e_i)^{\\top} A.$$\n",
    "\n",
    "Using the Sherman-Woodbury-Morrison formula for the inverse of the rank-$1$ update of the matrix\n",
    "\n",
    "we get\n",
    "\n",
    "$$\\widehat{A}^{-1}_{new} =  \\widehat{A}^{-1} - \\beta \\widehat{A}^{-1} e_i (e_j - e_i)^{\\top} A \\widehat{A}^{-1},$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\beta = \\frac{1}{1 + (A\\widehat{A} e_i, e_j - e_i)}.$$\n",
    "\n",
    "The final formulas come by multiplying \n",
    "\n",
    "$A_{new}$ by $\\widehat{A}_{new}^{-1}$ \n",
    "\n",
    "The total complexity of such update is $\\mathcal{O}(nr)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maxvol algorithm: final remarks\n",
    "- Typical number of iterations is 10\n",
    "- QR decomposition can help the stability \n",
    "- This is an extremely important algorithm that has been known in other areas for **decades**\n",
    "- The maximum-volume submatrix in a \"tall\" matrix gives us good interpolation points, \n",
    "  thus can be used for **optimal experiment design.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternating-maxvol summary\n",
    "\n",
    "The summary of the algorithm:\n",
    "\n",
    "Select $\\mathcal{I}$ at random\n",
    "\n",
    "$$\n",
    "   J = \\mathrm{maxvol}(A[I, :]^{\\top})\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ I = \\mathrm{maxvol}(A[:, J]). $$\n",
    "\n",
    "In this case, the volume never decreases, and $2-3$ iterations are often needed to find the right matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adapting the rank\n",
    "\n",
    "The problem of the alternating Maxvol algorithm is that the number of columns (approximate rank) has to be fixed.\n",
    "\n",
    "It is typically not known in advance.\n",
    "\n",
    "Thus, **greedy** algorithms have been developed, based on the idea of **cross approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross approximation and rank reduction\n",
    "\n",
    "Cross approximation is just rank-$1$ skeleton decomposition.\n",
    "\n",
    "Take column with index $j_*$, row with index $i_*$ and compute\n",
    "\n",
    "\n",
    "$$A'_{ij} = A_{ij} - \\frac{A_{ij_*} A_{i_* j}}{A_{i_*j_*}}.$$\n",
    "\n",
    "The only requirement is that $A_{i_*, j_*}$ is not equal to $0$.\n",
    "\n",
    "Then, the matrix $A'$ coincides with $A$ on the **cross** comprised from the $i_*$ row and $j_*$ column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross approximation and rank reduction(2)\n",
    "\n",
    "The subtraction of the cross leads to the rank reduction:\n",
    "\n",
    "$$\\mathrm{rank}(A') = \\mathrm{rank}(A) - 1,$$\n",
    "\n",
    "i.e. if the matrix $A$ had rank $1$, after $r$ such steps we will have zero matrix.\n",
    "\n",
    "Moreover, each cross is a **rank-1** matrix, thus\n",
    "\n",
    "$$A_r = 0 = A_{r-1} - u_{r-1} v^{\\top}_{r-1} = \\ldots = A - \\sum_{k=1}^r u_k v^{\\top}_k, $$\n",
    "\n",
    "thus\n",
    "\n",
    "$$A = \\sum_{k=1}^r u_k v^{\\top}_k,$$\n",
    "\n",
    "i.e. the required low-rank approximation.\n",
    "\n",
    "Also, the cross of $A_k$ at the $k$-th can be computed cheaply, since\n",
    "\n",
    "$$A_k = A - UV^{\\top},$$\n",
    "\n",
    "where $U$ and $V$ have $k$ columns, \n",
    "\n",
    "and \n",
    "\n",
    "$$A_k e_j = A e_j - U V^{\\top} e_j$$ can be computed by computing the column of $A$ and $nk$ additional operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selection of pivots\n",
    "\n",
    "Selection of pivots $(i_*, j_*)$ is the crucial point; \n",
    "\n",
    "The larger is the modulus, the better is the approximation, \n",
    "\n",
    "i.e. we have to find large values in the remainder.\n",
    "\n",
    "One of the ways is to do **partial pivoting**.\n",
    "\n",
    "If you do row pivoting (i.e. fix $i_*$ and find $j_*$ as the maximal element in a row)\n",
    "\n",
    "the cross can be computed in a stable way since\n",
    "\n",
    "$$\\left|\\frac{a_{ij_*}}{a_{i_* j_*}}\\right| \\leq 1, $$\n",
    "\n",
    "and it is not difficult to see that the error will grow at most by a factor of $2$ at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More robust pivoting\n",
    "\n",
    "More robust pivoting come from the Gaussian elimination:\n",
    "\n",
    "- Rook pivoting (coordinate descent)\n",
    "- Active submatrices (basically, redoing maxvol after $k$ steps)\n",
    "\n",
    "The main reason why such methods work is that being shown $r$ elements from the row we are able to determine,\n",
    "\n",
    "which are mostly linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CA and Wedderburn\n",
    "\n",
    "One step of CA can be written as a rank-1 update of the matrix:\n",
    "\n",
    "$$A_{k+1} = A_k - \\frac{A e_i e^{\\top}_j A}{(A e_i, e_j)}.$$\n",
    "\n",
    "This is an example of a more general class of Wedderburn updates of the matrix:\n",
    "\n",
    "$$A_{k+1} = A_k - \\frac{A u v^{\\top} A}{(A u, v)}.$$\n",
    "\n",
    "For any pair of $(u, v)$ they reduce the rank, and the best \"pivot\" corresponds to the \n",
    "maximum of $|(Au, v)|$ which is given by the singular vector corresponding to the largest singular value.\n",
    "\n",
    "Moreover, any matrix decomposition (LU, QR) can be obtained from this formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note on the theory\n",
    "\n",
    "There is not too much theory on why these methods work.\n",
    "\n",
    "For any strategy, there are examples, where they will not work.\n",
    "\n",
    "For a class of function-related matrices there is a result by Demanet et. al\n",
    "\n",
    "who showed that if the matrix $A$ can be written as\n",
    "\n",
    "$$A = U \\Phi V^{\\top} + E,$$\n",
    "\n",
    "and $U^{\\top} U = V^{\\top} V = I_r$\n",
    "\n",
    "and $U$ and $V$ satisfy **incoherence properties**\n",
    "\n",
    "$$\\max |U_{ij}| \\leq \\frac{\\mu}{\\sqrt{N}}, \\quad \\max |V_{ij}| \\leq \\frac{\\mu}{\\sqrt{M}},$$\n",
    "\n",
    "(it means, that there are no **gaps** in singular values) then it is sufficient to pick any \n",
    "\n",
    "$$p = r \\log n$$ columns.\n",
    "\n",
    "Still a lot to be done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary \n",
    "Today we talked about:\n",
    "\n",
    "- A little bit on FMM from algebra part & estimates\n",
    "- Adaptive cross approximation & maxvol algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Working with H- and H-2 matrices: addition, multiplication, inversion\n",
    "- Connection to sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
