{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.html.services.config import ConfigManager\n",
    "from IPython.utils.path import locate_profile\n",
    "cm = ConfigManager(profile_dir=locate_profile(get_ipython().profile))\n",
    "cm.update('livereveal', {\n",
    "              'theme': 'sky',\n",
    "              'transition': 'zoom',\n",
    "              'start_slideshow_at': 'selected',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 11. Iterative methods (advanced topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (Approximate) Syllabus\n",
    "- **Week 1:** Intro & basic integral equations (turning PDEs into IEs, typical kernels, Nystrom, collocation, Galerkin, quadrature for singular/hypersingular integrals).\n",
    "- **Week 2:** Translation-invariant kernels and convolutions, FFT. Concept of close and far interactions precorrected FFT. Barnes-Hut method\n",
    "- **Week 3:**  Fast multipole methods. Algebraic analogue of fast multipole method, hierarchical matrices\n",
    "- **Week 4:**  Multigrid methods, domain decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Previous lecture: \n",
    "\n",
    "Theory of geometric multigrid, subspace correction iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Todays lecture\n",
    "\n",
    "Today we will talk a little about the iterative methods (most of it was covered in the NLA course) refresh the basic stuff and cover some advanced topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The\"philosophy\" of iterative methods\n",
    "As you remember, and iterative method is the computation of the form\n",
    "$$x_{k+1} = \\Phi(x_k), \\quad k = 1, \\ldots$$\n",
    "If the mapping $\\Phi(x)$ is contractive, then \n",
    "the iterative process converges to the unique stationary point\n",
    "$$x_* = \\Phi(x_*).$$\n",
    "What are the additional requirements on $\\Phi$ (and how this theorem is proved)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed-point convergence\n",
    "\n",
    "**Theorem**: Let $M$ be a full metric space with distance $\\rho$. Then, if the mapping $\\Phi: M \\rightarrow M$ is a contraction mapping, i.e. \n",
    "$$\n",
    "   \\rho(\\Phi(x), \\Phi(y)) \\leq q \\rho(x, y),\n",
    "$$\n",
    "where $q < 1$\n",
    "the fixed-point method converges to a unique solution of \n",
    "\n",
    "$$x = \\Phi(x).$$\n",
    "\n",
    "The proof is based on the observation that $x_k$ is a Cauchy sequence:\n",
    "\n",
    "For any $m \\geq k$\n",
    "\n",
    "$$\\rho(x_m, x_k) \\leq \\sum_{i = k}^{m-1} \\rho(x_{i+1}, x_i) \\leq \\sum_{i=k}^{m-1} q^k \\rho(x_1, x_0) \\leq \\rho(x_1, x_0) \\frac{q^k}{1-q}.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reminder: simple iteration\n",
    "For linear systems, we have the simplest iterative method\n",
    "    $$x_{k+1} = x_k + B (f - Ax_k),$$\n",
    " and it converges if $\\rho (I - B A) < 1$, where $\\rho$ is a spectral radius.\n",
    " To prove the convergence under the condition, we need to use the Jordan form.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding up the convergence of the fixed-point iteration\n",
    "There are many way to speed-up the convergence of fixed-point iteration method.\n",
    "\n",
    "Newton method is probably one of the most well-known for a non-linear case, \n",
    "\n",
    "when we rewrite the equation \n",
    "\n",
    "$$f(x) = 0$$\n",
    "\n",
    "in the form\n",
    "\n",
    "$$g(x) = x, $$\n",
    "\n",
    "where $g(x) = x - \\alpha(x) f(x),$$\n",
    "\n",
    "and the best local convergence is attained by setting\n",
    "\n",
    "$$\\alpha(x) = (f'(x))^{-1}$$.\n",
    "\n",
    "This leads to the solution of a linear system, thus not very useful for our purposes (although for non-linear PDE it might be useful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anderson acceleration\n",
    "\n",
    "Given a fixed-point iterations, \n",
    "\n",
    "$$x_{k+1} = g(x_k),$$\n",
    "\n",
    "how can we speed up the convergence?\n",
    "\n",
    "One of the most simple (but often very effective) ways to do so is the so-called **Anderson acceleration**\n",
    "\n",
    "Very widely used in chemistry, under the name \"Direct inversion in the iterated subspaces\" (DIIS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anderson acceleration: how to derive it\n",
    "\n",
    "In the standard fixed-point method we store only the previous iterate $x_k$.\n",
    "\n",
    "The idea of AA comes from storing a certain **history** of the iterates with depth $m$:\n",
    "\n",
    "Then we take a **weighted sum** (also called mixing) to approximate the new iterate:\n",
    "\n",
    "$$x_{k+1} = \\sum_{j = 0}^m \\alpha_j g(x_{k-j}).$$\n",
    "\n",
    "In order to make the fixed point the same, we need\n",
    "\n",
    "$$\\sum_{j=0}^m \\alpha_j = 1.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anderson acceleration: how to derive it (2)\n",
    "\n",
    "The main question is how select $\\alpha$. \n",
    "\n",
    "We are solving the equation \n",
    "\n",
    "$$x - g(x) = f(x) = 0.$$\n",
    "\n",
    "Then, $f(x_k)$ can be named **residual** at point $x_k$.\n",
    "\n",
    "Then, in the AA we select $\\alpha$ in such a way that\n",
    "\n",
    "$$ \\Vert \\sum_{\\alpha_j} f_{k-j}  \\Vert \\rightarrow \\min.$$\n",
    "\n",
    "Subject to the constraint $\\sum_{j=0}^m \\alpha_j = 1$ we have the linear least squares formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anderson acceleration: final formula\n",
    "\n",
    "The minimization problem can be rewritten in the equivalent form as\n",
    "\n",
    "$$ \\min_{\\beta_1, \\ldots, \\beta_k} \\Vert f_k - \\sum_{j=1}^m \\beta_j \\left(f_{k-j+1} - f_{k-j}\\right) \\Vert_2,$$\n",
    "\n",
    "without any additional restrictions on $\\beta$.\n",
    "\n",
    "Thus, we form a matrix $F$ with columns $f_k - f_{k-1}$ and solve a linear least squares problem with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo of simple iteration & AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it= 0, er= 4.19e-02\n",
      "it= 1, er= 3.49e-04\n",
      "it= 2, er= 5.96e-07\n",
      "[ 0.73908513]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def fixed_point(x0, f, eps=1e-6, niters=1000):\n",
    "    k = 0\n",
    "    er = 2 * eps\n",
    "    x = x0.copy()\n",
    "    while k < niters and er > eps:\n",
    "        x1 = f(x)\n",
    "        er = np.linalg.norm(x1 - x)\n",
    "        x = x1.copy()\n",
    "        print('it={0: d}, er={1: 3.2e}'.format(k, er))\n",
    "        k += 1\n",
    "        \n",
    "def anderson_acceleration(x1, f, eps=1e-6, m=5, warmup=5, niters=1000, condtol=1e12):\n",
    "    x0 = x1.copy()\n",
    "    for i in range(warmup):\n",
    "        x0 = f(x0)\n",
    "    x = f(x0)\n",
    "    xc = np.hstack((x0, x))\n",
    "    res = np.hstack((f(x0) - x0, f(x) - x))\n",
    "    #xc = [x0, x]\n",
    "\n",
    "    k = 0\n",
    "    er = 2 * eps\n",
    "    while k < niters and er > eps:\n",
    "        er = np.linalg.norm(res[:, -1])\n",
    "        print('it={0: d}, er={1: 3.2e}'.format(k, er))\n",
    "        if np.size(res, 1) > m or np.linalg.cond(res) > condtol:\n",
    "            res = res[:, 1:]\n",
    "            xc = xc[:, 1:]\n",
    "        m0 = np.size(res, 1)\n",
    "        ed = np.eye(m0)[:m0-1, :]\n",
    "        zd = np.eye(m0)[1:, :]\n",
    "        gd = zd - ed\n",
    "        Fk = res.dot(gd.T)\n",
    "        rhs = res[:, -1]\n",
    "        gm = np.linalg.lstsq(Fk, rhs)[0]\n",
    "        dx = xc.dot(gd.T)\n",
    "        xnew = rhs + xc[:, -1] - (dx + Fk).dot(gm)\n",
    "        xnew = np.reshape(xnew, (xc.shape[0], -1))\n",
    "        xc = np.hstack((xc, xnew))\n",
    "        res = np.hstack((res, f(xnew) - xnew))\n",
    "        k += 1\n",
    "    sol = xc[:, -1]\n",
    "    return sol\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "x0 = np.ones((1, 1))\n",
    "sol = anderson_acceleration(x0, f)\n",
    "print sol\n",
    "\n",
    "#sol_fixed = fixed_point(x0, f)\n",
    "#print sol_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Anderson acceleration & GMRES\n",
    "Anderson acceleration applied to linear system acceleration is the **Generalized** minimal residual method.\n",
    "\n",
    "In this case, we have $$g(x) = Ax + b.$$\n",
    "\n",
    "AA without truncation is equivalent to the GMRES method applied to the linear system \n",
    "\n",
    "$$(I - A) x = b$$ starting with the same $x_0$. For details see the paper by [Walker et. al](http://users.wpi.edu/~walker/Papers/Walker-Ni,SINUM,V49,1715-1735.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Krylov subspaces \n",
    "\n",
    "To solve $Ax = b$ we take random initial vector $x_0$ and solve\n",
    "\n",
    "$$A y = r_0 = b - A x_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Krylov subspaces & GMRES\n",
    "\n",
    "Now it is probably a good idea to recall the basic idea of Krylov methods and GMRES.\n",
    "\n",
    "In GMRES, we look for the solution in the Krylov subspace\n",
    "\n",
    "$$\\mathcal{K}_i = \\{b, Ab, A^2 b, \\ldots, A^{i} b\\}$$ that minimizes the residual\n",
    "\n",
    "$$\\Vert A x - b \\Vert $$\n",
    "\n",
    "over $K_i$\n",
    "\n",
    "The length of the vector\n",
    "\n",
    "$$r_i = b - Ay $$ is minimal only if \n",
    "\n",
    "$$r_i \\perp A \\mathcal{K}_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometric version of GMRES\n",
    "\n",
    "In the **geometric versison** of GMRES we construct $q_1, \\ldots, q_i$ form the basis in $\\mathcal{K}_i$, and another basis $p_1 = A q_1, \\ldots, p_i = A q_i$ such that they form orthogonal basis in $A \\mathcal{K}_i$. \n",
    "\n",
    "A vector $q_i$ from this sequence should satisfy the following properties:\n",
    "\n",
    "$$q_{i+1} \\notin \\mathcal{K}_i, \\quad q_{i+1}  \\in \\mathcal{K}_{i+1}, \\quad p_{i+1} = A q_{i+1} \\perp A \\mathcal{K}_i.$$\n",
    "\n",
    "This vector can be obtained by applying the orthogonalization procedure to the vector $p = Aq$, where $q = Aq_i$.\n",
    "\n",
    "In the geometric version one needs to store both $q$ and $p$ (two sequences of vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algebraic version of GMRES\n",
    "\n",
    "Thw algebraic approach uses only one sequence of vectors $q_1, \\ldots, q_i$, such that they form the basis in $\\mathcal{K}_i$.\n",
    "\n",
    "This idea comes from the paper by Saad (where the widely used GMRES acronym comes as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We start from \n",
    "$$q_0 = \\frac{r_0}{\\Vert r_0 \\Vert}.$$\n",
    "\n",
    "In order to  build a basis, we build the last vector  and orthogonalize it to the previous one:\n",
    "\n",
    "$$q_{i+1}= \\frac{\\widehat{q}_{i+1}}{\\Vert \\widehat{q}_{i+1} \\Vert}, \\quad \\widehat{q}_{i+1} = A q_i - h_{i i} q_i - \\ldots - h_{i1} q_1,$$\n",
    "\n",
    "where $h_{ik} = (A q_i, q_k), h_{i+1 i} = \\Vert \\widehat{q}_{i+1} \\Vert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix form\n",
    "\n",
    "In the matrix form we have the following decomposition:\n",
    "\n",
    "$$Aq_i=\\begin{bmatrix} q_1 & \\ldots & q_{i+1} \\end{bmatrix} \\begin{bmatrix} h_{i1} \\\\ \\vdots \\\\ h_{i+1 i} \\end{bmatrix}$$\n",
    "\n",
    "Thus, after the first $i$ steps we have\n",
    "\n",
    "$$AQ_i = Q_{i+1}  \\widehat{H}_i,$$\n",
    "\n",
    "where $\\widehat{H}_i$ has the form\n",
    "\n",
    "$$\\widehat{H}_i = \\begin{bmatrix} H_i \\\\ 0 & \\ldots & h_{i+1 i} \\end{bmatrix},$$\n",
    "\n",
    "where $H_i$ is upper Hessenberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix form (cont.)\n",
    "\n",
    "Let $$\\widehat{H}_i = U_i R_i, \\quad U_i \\in \\mathbb{C}^{(i+1) \\times i}, \\quad R_i \\in \\mathbb{C}^{i \\times i},$$\n",
    "be the QR-factorization of $\\widehat{H}_i$.\n",
    "\n",
    "The minimization problem \n",
    "\n",
    "$$\\Vert r_0 - A y \\Vert_2 \\rightarrow \\min, \\quad y \\in \\mathcal{K}_i$$\n",
    "\n",
    "is equivalent to solving the least square problem\n",
    "\n",
    "$$\\Vert r_0 - A Q_i y \\Vert_2.$$\n",
    "\n",
    "The **minimum** is obtained for $y_i$ that solves\n",
    "\n",
    "$$R_i y_i = \\Vert r_0 \\Vert^2 U^*_i e_1.$$\n",
    "\n",
    "\n",
    "Finally, $$x_i = x_0 + Q_i y_i = x_0 + Q_i R^{-1}_i z_i.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note on the implementation\n",
    "\n",
    "We need to orthogonalize $\\widehat{H}_i$.\n",
    "\n",
    "Als, the vectors $z_i$  and $y_i$ can be computed in $\\mathcal{O}(i^2)$ operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of GMRES\n",
    "\n",
    "- We need to store $i$ vectors at each iteration. \n",
    "- Only the product $Ax$ is needed\n",
    "\n",
    "If we need to store constant number of vectors, we need either:\n",
    "\n",
    "- Special matrices (symmetric positive definite case works very well).\n",
    "- BiCGStab method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symmetric positive definite case: CG method\n",
    "\n",
    "If $A = A%* > 0$ we have the conjugate gradient method, which allows short storage and has wonderful convergence properties.\n",
    "\n",
    "Again, given $x_0$ we solve\n",
    "\n",
    "$$Au_0 = r_0,$$\n",
    "\n",
    "and look for the solution that minimizes the $A$-norm of the error, i.e.\n",
    "\n",
    "$$\\Vert y - u_0 \\Vert_A$$ \n",
    "over all vector in $\\mathcal{K}_i$.\n",
    "\n",
    "\n",
    "This gives short-term recurrent relations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Krylov subspaces\n",
    "\n",
    "We have used quite specific subspaces (**Krylov subspaces**) to represent the solution.\n",
    "\n",
    "Maybe it is better to compute matrix-by-vector products with other vectors, (i.e. random ones?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal subspaces\n",
    "\n",
    "Consider and algorithm $\\Phi$ that produces a sequence of **embedded subspaces**\n",
    "\n",
    "$$L_{i+1} = \\Phi(b, L_i, AL_i).$$\n",
    "\n",
    "The quality of the algorithm is characterized as its index by.\n",
    "\n",
    "$$m(\\Phi, A, b) = \\min \\{i: \\min_y \\Vert A y - b \\Vert_2 \\leq \\epsilon.$$\n",
    "\n",
    "Bad algorithm can have infinity index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nemirovsky and Judin result\n",
    "\n",
    "In the late 1970 Judin & Nemirosvsky result on the quasi-optimality of the Krylov subspaces.\n",
    "t\n",
    "The result states: For any non-singular matrix $A$ there exists a unitary matrix $Q$ such that\n",
    "\n",
    "$$m(\\Phi_k, a, b) \\leq 2 m(\\Phi, Q^* A Q, b) + 1,$$\n",
    "\n",
    "where $\\Phi_k$ is the algorithm that generates Krylov subspaces.\n",
    "\n",
    "This gives you the quasioptimality of the Krylov s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preconditioning\n",
    "\n",
    "Solve $B^{-1} A x = Bf$, instead of $Ax = f$ with the hope that the matrix $B^{-1} A$ is **better conditioned**.\n",
    "\n",
    "Remember the difference between right & left preconditioners\n",
    "\n",
    "$$BA x = Bf, \\quad A B y = f$$\n",
    "\n",
    "and explicit and implicit preconditioners\n",
    "\n",
    "$$BA x = Bf, \\quad B^{-1} A x = B^{-1} f.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What can we use for preconditioning?\n",
    "\n",
    "As a preconditioner, many different things can be used:\n",
    "\n",
    "\n",
    "For sparse linear systems:\n",
    "\n",
    "1. Incomplete LU preconditioners (ILUT, ILU(k), ILU2)\n",
    "2. Multigrid preconditioners (both algebraic and geometric multigrid)\n",
    "3. Domain decomposition (tomorrow)\n",
    "4. H-matrices factorizations as approximate inverses (i.e. inverse operators for many PDEs can be well approximated by H-matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preconditioners for dense matrices\n",
    "\n",
    "For dense matrices, for a shift-invariant case, circulant matrices (as easily inverted by the FFT).\n",
    "\n",
    "For a general hierarhical matrix, its inverse can be approximated by the H-matrix as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Anderson acceleration\n",
    "- GMRES\n",
    "- And fast solvers for dense matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "- Domain decomposition (additive Schwarz, multiplicative Schwarz)\n",
    "- Parallelization of fast sparse solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Fenix' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:300,400' rel='stylesheet' type='text/css'>\n",
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        /*width:80%;*/\n",
       "        /*margin-left:auto !important;\n",
       "        margin-right:auto;*/\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\n",
       "    h2 {\n",
       "        font-family: 'Fenix', serif;\n",
       "    }\n",
       "    h3{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "\th4{\n",
       "\t\tfont-family: 'Fenix', serif;\n",
       "       }\n",
       "    h5 {\n",
       "        font-family: 'Alegreya Sans', sans-serif;\n",
       "    }\t   \n",
       "    div.text_cell_render{\n",
       "        font-family: 'Alegreya Sans',Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 1.2;\n",
       "        font-size: 120%;\n",
       "        /*width:70%;*/\n",
       "        /*margin-left:auto;*/\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\";\n",
       "\t\t\tfont-size: 90%;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 50pt;\n",
       "\t\tline-height: 110%;\n",
       "        color:#CD2305;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\t\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #CD2305;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "    \n",
       "    li {\n",
       "        line-height: 110%;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }  \n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
